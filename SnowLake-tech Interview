一开始就是很普通的自我介绍的一个过程，对之前的项目经历进行几个提问，可能我做的主要是图像分类的问题，与面试的这个公司做的目标检测没什么用。面试官粗略地问了一下，就没接着问下去。
- 接下来开始问做的项目中用的什么损失函数？还知道哪些损失函数？用过哪些激活函数？还知道哪些激活函数？
我用的是比较常见的CrossEntrpy。均方差损失函数，SVM合页损失，Smooth L1损失。激活函数：Sigmoid、tanh、ReLu、Leaky ReLu、ELU (Exponential Linear Units)  、MaxOut
- Softmax 与CrossEntrpy的区别是什么
这个没有答上来。softmax loss是由softmax和交叉熵(cross-entropy loss)组合而成，全称是softmax with cross-entropy loss，所以我们可以想见，它们是不同的，但是又有关系。

解答1：首先我们得知道什么是交叉熵。

在物理学有一个概念，就是熵，它表示一个热力学系统的无序程度。为了解决对信息的量化度量问题，香农在1948年提出了“信息熵”的概念，它使用对数函数表示对不确定性的测量。熵越高，表示能传输的信息越多，熵越少，表示传输的信息越少，我们可以直接将熵理解为信息量。

按照香农的理论，熵背后的原理是任何信息都存在冗余，并且冗余大小与信息中每个符号（数字、字母或单词）的出现概率或者说不确定性有关。概率大，出现机会多，则不确定性小，这个关系就用对数函数来表征。

为什么选择对数函数而不是其他函数呢？首先，不确定性必须是概率P的单调递降函数，假设一个系统中各个离散事件互不相关，要求其总的不确定性等于各自不确定性之和，对数函数是满足这个要求的。将不确定性f定义为log(1/p)=-log(p)，其中p是概率。

对于单个的信息源，信源的平均不确定性就是单个符号不确定性-logpi的统计平均值，信息熵的定义如下。
![image.png](http://codewyf.top/upload/2020/07/image-b547996c669342b7ab42f759015b6fb8.png)


假设有两个概率分布p(x)和q(x)，其中p是已知的分布，q是未知的分布，则其交叉熵函数是两个分布的互信息，可以反应其相关程度。

从这里，就引出了分类任务中最常用的loss，即log loss，又名交叉熵loss，后面我们统一称为交叉熵loss，它的定义形式如下：
![image.png](http://codewyf.top/upload/2020/07/image-7a09dba7715b4d3dbf8cbdf7d8f0e1d3.png)


n对应于样本数量，m是类别数量，yij 表示第i个样本属于分类j的标签，它是0或者1。对于单分类任务，只有一个分类的标签非零。f(xij) 表示的是样本i预测为j分类的概率。loss的大小，完全取决于分类为正确标签那一类的概率，当所有的样本都分类正确时，loss=0，否则大于0。

解答2：假如log loss中的f(xij)的表现形式是softmax概率的形式，那么交叉熵loss就是我们熟知的softmax with cross-entropy loss，简称softmax loss，所以说softmax loss只是交叉熵的一个特例。

- 最后是一个简单的python的输出问题
```python
a='python
b= a[::-2]
         
print(b) 
#nhy

```
总结了一下还是需要再去系统学一遍python以及数据结构与算法，深度学习基本原理
